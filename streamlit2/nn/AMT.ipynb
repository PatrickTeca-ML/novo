{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3323ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee108525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#mysql_jar_path = os.path.abspath(\"mysql-connector-j-9.0.0.jar\")\n",
    "#print(\"Caminho completo do JAR:\", mysql_jar_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785d8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 00:07:16.166461: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 00:07:16.430725: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 00:07:16.719977: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747440436.858770    3419 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747440436.884773    3419 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747440437.083500    3419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747440437.083555    3419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747440437.083559    3419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747440437.083562    3419 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-17 00:07:17.106660: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414f509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd640b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in ./venv/lib/python3.12/site-packages (0.2.61)\n",
      "Requirement already satisfied: pandas>=1.3.0 in ./venv/lib/python3.12/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.31 in ./venv/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in ./venv/lib/python3.12/site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in ./venv/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in ./venv/lib/python3.12/site-packages (from yfinance) (3.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in ./venv/lib/python3.12/site-packages (from yfinance) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.11.1)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in ./venv/lib/python3.12/site-packages (from yfinance) (5.29.4)\n",
      "Requirement already satisfied: websockets>=13.0 in ./venv/lib/python3.12/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
      "Requirement already satisfied: cffi>=1.12.0 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944da4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582e27cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open   Volume\n",
      "Ticker             AMT         AMT         AMT         AMT      AMT\n",
      "Date                                                               \n",
      "2018-01-02  118.035690  119.783683  117.851690  119.733503  1880600\n",
      "2018-01-03  118.495628  119.181451  117.809819  117.968730  1770500\n",
      "2018-01-04  116.764412  118.729866  116.739322  118.294955  1729600\n",
      "2018-01-05  117.517120  117.676031  116.747670  117.282940  2119400\n",
      "2018-01-08  118.771645  119.800378  117.642565  117.642565  1882100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ticker = \"AMT\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebe3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_AMT = f\"{ticker}_AMT_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_AMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4284f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMT= pd.read_csv(\"AMT_AMT_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8781aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>118.03569030761719</td>\n",
       "      <td>119.78368259804581</td>\n",
       "      <td>117.85168977579708</td>\n",
       "      <td>119.73350295516077</td>\n",
       "      <td>1880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>118.4956283569336</td>\n",
       "      <td>119.18145054894164</td>\n",
       "      <td>117.80981892678248</td>\n",
       "      <td>117.96872956941152</td>\n",
       "      <td>1770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>116.76441192626953</td>\n",
       "      <td>118.72986625977379</td>\n",
       "      <td>116.73932210600027</td>\n",
       "      <td>118.29495477200356</td>\n",
       "      <td>1729600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price               Close                High                 Low  \\\n",
       "0      Ticker                 AMT                 AMT                 AMT   \n",
       "1        Date                 NaN                 NaN                 NaN   \n",
       "2  2018-01-02  118.03569030761719  119.78368259804581  117.85168977579708   \n",
       "3  2018-01-03   118.4956283569336  119.18145054894164  117.80981892678248   \n",
       "4  2018-01-04  116.76441192626953  118.72986625977379  116.73932210600027   \n",
       "\n",
       "                 Open   Volume  \n",
       "0                 AMT      AMT  \n",
       "1                 NaN      NaN  \n",
       "2  119.73350295516077  1880600  \n",
       "3  117.96872956941152  1770500  \n",
       "4  118.29495477200356  1729600  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AMT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0749348f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 00:10:21 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|               AMT|               AMT|               AMT|               AMT|    AMT|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02|118.03569030761719|119.78368259804581|117.85168977579708|119.73350295516077|1880600|\n",
      "|2018-01-03| 118.4956283569336|119.18145054894164|117.80981892678248|117.96872956941152|1770500|\n",
      "|2018-01-04|116.76441192626953|118.72986625977379|116.73932210600027|118.29495477200356|1729600|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Criar nova SparkSession com o caminho exato do JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do CSV\n",
    "file_path = \"file:///home/hduser/novo/AMT_AMT_historical_train_data_spark.csv\"\n",
    "\n",
    "# Leitura do CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Visualizar as primeiras 5 linhas\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1410318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab506793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|               AMT|               AMT|               AMT|               AMT|    AMT|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02|118.03569030761719|119.78368259804581|117.85168977579708|119.73350295516077|1880600|\n",
      "|2018-01-03| 118.4956283569336|119.18145054894164|117.80981892678248|117.96872956941152|1770500|\n",
      "|2018-01-04|116.76441192626953|118.72986625977379|116.73932210600027|118.29495477200356|1729600|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3274be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9685426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3960943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|      Date|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|2018-01-02|118.03569030761719|119.78368259804581|117.85168977579708|119.73350295516077|1880600|\n",
      "|2018-01-03| 118.4956283569336|119.18145054894164|117.80981892678248|117.96872956941152|1770500|\n",
      "|2018-01-04|116.76441192626953|118.72986625977379|116.73932210600027|118.29495477200356|1729600|\n",
      "|2018-01-05|117.51712036132812|117.67603104845792| 116.7476695053117|117.28294022088228|2119400|\n",
      "|2018-01-08|118.77164459228516|119.80037801502843|117.64256467527281|117.64256467527281|1882100|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d194d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35d95115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89b561a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d99cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e204efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
    "df = df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
    "df = df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
    "df = df.withColumn(\"Volume\", col(\"Volume\").cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5822132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f03d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "581cf41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|      Date|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|2018-01-02|118.03569030761719|119.78368259804581|117.85168977579708|119.73350295516077|1880600.0|\n",
      "|2018-01-03| 118.4956283569336|119.18145054894164|117.80981892678248|117.96872956941152|1770500.0|\n",
      "|2018-01-04|116.76441192626953|118.72986625977379|116.73932210600027|118.29495477200356|1729600.0|\n",
      "|2018-01-05|117.51712036132812|117.67603104845792| 116.7476695053117|117.28294022088228|2119400.0|\n",
      "|2018-01-08|118.77164459228516|119.80037801502843|117.64256467527281|117.64256467527281|1882100.0|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17fdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16b187a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ee5abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Leitura do CSV\n",
    "#df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Adiciona a coluna 'nome' com valor fixo \"AAPL\"\n",
    "df_comp_nome = df.withColumn(\"nome\", lit(\"AMT\"))\n",
    "\n",
    "# Escreve no banco MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"AMT_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2777b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163831",
   "metadata": {},
   "source": [
    "mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62e30d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='password',\n",
    "    database='companies'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM AAPL_historico\", conn)\n",
    "df.to_csv(\"AAPL_historico_MYsql.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ABNB_Mysql= pd.read_csv(\"ABNB_historico_MYsql.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be36a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL_Mysql.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bdsp2025)",
   "language": "python",
   "name": "venv-bdsp2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
