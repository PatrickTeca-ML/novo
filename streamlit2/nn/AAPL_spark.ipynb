{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3323ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee108525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#mysql_jar_path = os.path.abspath(\"mysql-connector-j-9.0.0.jar\")\n",
    "#print(\"Caminho completo do JAR:\", mysql_jar_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785d8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 22:44:31.591201: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 22:44:31.826811: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 22:44:32.027607: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747521872.119025    3073 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747521872.179897    3073 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747521872.384212    3073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747521872.384240    3073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747521872.384242    3073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747521872.384243    3073 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-17 22:44:32.404971: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414f509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd640b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in ./venv/lib/python3.12/site-packages (0.2.61)\n",
      "Requirement already satisfied: pandas>=1.3.0 in ./venv/lib/python3.12/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.31 in ./venv/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in ./venv/lib/python3.12/site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in ./venv/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in ./venv/lib/python3.12/site-packages (from yfinance) (3.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in ./venv/lib/python3.12/site-packages (from yfinance) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.11.1)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in ./venv/lib/python3.12/site-packages (from yfinance) (5.29.4)\n",
      "Requirement already satisfied: websockets>=13.0 in ./venv/lib/python3.12/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
      "Requirement already satisfied: cffi>=1.12.0 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944da4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582e27cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price           Close       High        Low       Open     Volume\n",
      "Ticker           AAPL       AAPL       AAPL       AAPL       AAPL\n",
      "Date                                                             \n",
      "2018-01-02  40.426815  40.436204  39.722760  39.933979  102223600\n",
      "2018-01-03  40.419788  40.964259  40.356426  40.490195  118071600\n",
      "2018-01-04  40.607525  40.710787  40.384575  40.492528   89738400\n",
      "2018-01-05  41.069866  41.156698  40.612231  40.703758   94640000\n",
      "2018-01-08  40.917313  41.213014  40.818742  40.917313   82271200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ticker = \"AAPL\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebe3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_AAPL = f\"{ticker}_AAPL_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_AAPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4284f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL= pd.read_csv(\"AAPL_AAPL_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8781aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>40.426815032958984</td>\n",
       "      <td>40.436204433265914</td>\n",
       "      <td>39.72276041223238</td>\n",
       "      <td>39.93397894705455</td>\n",
       "      <td>102223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>40.4197883605957</td>\n",
       "      <td>40.96425916920155</td>\n",
       "      <td>40.356426001755686</td>\n",
       "      <td>40.49019456253724</td>\n",
       "      <td>118071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>40.60752487182617</td>\n",
       "      <td>40.710786792933035</td>\n",
       "      <td>40.384574949985684</td>\n",
       "      <td>40.492527990410416</td>\n",
       "      <td>89738400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price               Close                High                 Low  \\\n",
       "0      Ticker                AAPL                AAPL                AAPL   \n",
       "1        Date                 NaN                 NaN                 NaN   \n",
       "2  2018-01-02  40.426815032958984  40.436204433265914   39.72276041223238   \n",
       "3  2018-01-03    40.4197883605957   40.96425916920155  40.356426001755686   \n",
       "4  2018-01-04   40.60752487182617  40.710786792933035  40.384574949985684   \n",
       "\n",
       "                 Open     Volume  \n",
       "0                AAPL       AAPL  \n",
       "1                 NaN        NaN  \n",
       "2   39.93397894705455  102223600  \n",
       "3   40.49019456253724  118071600  \n",
       "4  40.492527990410416   89738400  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AAPL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0749348f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|     Price|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|    Ticker|              AAPL|              AAPL|              AAPL|              AAPL|     AAPL|\n",
      "|      Date|              null|              null|              null|              null|     null|\n",
      "|2018-01-02|40.426815032958984|40.436204433265914| 39.72276041223238| 39.93397894705455|102223600|\n",
      "|2018-01-03|  40.4197883605957| 40.96425916920155|40.356426001755686| 40.49019456253724|118071600|\n",
      "|2018-01-04| 40.60752487182617|40.710786792933035|40.384574949985684|40.492527990410416| 89738400|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Criar nova SparkSession com o caminho exato do JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do CSV\n",
    "file_path = \"file:///home/hduser/novo/AAPL_AAPL_historical_train_data_spark.csv\"\n",
    "\n",
    "# Leitura do CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Visualizar as primeiras 5 linhas\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1410318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab506793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|     Price|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|    Ticker|              AAPL|              AAPL|              AAPL|              AAPL|     AAPL|\n",
      "|      Date|              null|              null|              null|              null|     null|\n",
      "|2018-01-02|40.426815032958984|40.436204433265914| 39.72276041223238| 39.93397894705455|102223600|\n",
      "|2018-01-03|  40.4197883605957| 40.96425916920155|40.356426001755686| 40.49019456253724|118071600|\n",
      "|2018-01-04| 40.60752487182617|40.710786792933035|40.384574949985684|40.492527990410416| 89738400|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab3274be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9685426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3960943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|      Date|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|2018-01-02|40.426815032958984|40.436204433265914| 39.72276041223238| 39.93397894705455|102223600|\n",
      "|2018-01-03|  40.4197883605957| 40.96425916920155|40.356426001755686| 40.49019456253724|118071600|\n",
      "|2018-01-04| 40.60752487182617|40.710786792933035|40.384574949985684|40.492527990410416| 89738400|\n",
      "|2018-01-05| 41.06986618041992|41.156698465850205| 40.61223124489687| 40.70375823200148| 94640000|\n",
      "|2018-01-08| 40.91731262207031|41.213014298775086| 40.81874181549477| 40.91731262207031| 82271200|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95d194d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35d95115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89b561a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1d99cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e204efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
    "df = df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
    "df = df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
    "df = df.withColumn(\"Volume\", col(\"Volume\").cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5822132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f03d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "581cf41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+----------+\n",
      "|      Date|             Close|              High|               Low|              Open|    Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+----------+\n",
      "|2018-01-02|40.426815032958984|40.436204433265914| 39.72276041223238| 39.93397894705455|1.022236E8|\n",
      "|2018-01-03|  40.4197883605957| 40.96425916920155|40.356426001755686| 40.49019456253724|1.180716E8|\n",
      "|2018-01-04| 40.60752487182617|40.710786792933035|40.384574949985684|40.492527990410416| 8.97384E7|\n",
      "|2018-01-05| 41.06986618041992|41.156698465850205| 40.61223124489687| 40.70375823200148|   9.464E7|\n",
      "|2018-01-08| 40.91731262207031|41.213014298775086| 40.81874181549477| 40.91731262207031| 8.22712E7|\n",
      "+----------+------------------+------------------+------------------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17fdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16b187a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ee5abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 12:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Leitura do CSV\n",
    "#df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Adiciona a coluna 'nome' com valor fixo \"AAPL\"\n",
    "df_comp_nome = df.withColumn(\"nome\", lit(\"AAPL\"))\n",
    "\n",
    "# Escreve no banco MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"AAPL_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2777b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163831",
   "metadata": {},
   "source": [
    "mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443fb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymysql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b72c52bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3073/971793236.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM AAPL_historico\", conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='password',\n",
    "    database='companies'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM AAPL_historico\", conn)\n",
    "df.to_csv(\"AAPL_historico_MYsql.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1259451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL_Mysql= pd.read_csv(\"AAPL_historico_MYsql.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea6aae2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>nome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>40.426815</td>\n",
       "      <td>40.436204</td>\n",
       "      <td>39.722760</td>\n",
       "      <td>39.933979</td>\n",
       "      <td>102223600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>40.419788</td>\n",
       "      <td>40.964259</td>\n",
       "      <td>40.356426</td>\n",
       "      <td>40.490195</td>\n",
       "      <td>118071600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>40.607525</td>\n",
       "      <td>40.710787</td>\n",
       "      <td>40.384575</td>\n",
       "      <td>40.492528</td>\n",
       "      <td>89738400.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>41.069866</td>\n",
       "      <td>41.156698</td>\n",
       "      <td>40.612231</td>\n",
       "      <td>40.703758</td>\n",
       "      <td>94640000.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>40.917313</td>\n",
       "      <td>41.213014</td>\n",
       "      <td>40.818742</td>\n",
       "      <td>40.917313</td>\n",
       "      <td>82271200.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Close       High        Low       Open       Volume  nome\n",
       "0  2018-01-02  40.426815  40.436204  39.722760  39.933979  102223600.0  AAPL\n",
       "1  2018-01-03  40.419788  40.964259  40.356426  40.490195  118071600.0  AAPL\n",
       "2  2018-01-04  40.607525  40.710787  40.384575  40.492528   89738400.0  AAPL\n",
       "3  2018-01-05  41.069866  41.156698  40.612231  40.703758   94640000.0  AAPL\n",
       "4  2018-01-08  40.917313  41.213014  40.818742  40.917313   82271200.0  AAPL"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AAPL_Mysql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b7c46",
   "metadata": {},
   "source": [
    "## cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58447b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: cassandra-driver 3.29.2\r\n",
      "Uninstalling cassandra-driver-3.29.2:\r\n",
      "  Would remove:\r\n",
      "    /home/hduser/novo/venv/lib/python3.12/site-packages/cassandra/*\r\n",
      "    /home/hduser/novo/venv/lib/python3.12/site-packages/cassandra_driver-3.29.2.dist-info/*\r\n",
      "    /home/hduser/novo/venv/lib/python3.12/site-packages/cassandra_driver.libs/libev-a3026d2b.so.4.0.0\r\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "pip uninstall cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92961f54",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoHostAvailable",
     "evalue": "('Unable to connect to any servers', {'127.0.0.1:9042': ConnectionRefusedError(111, \"Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\")})",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoHostAvailable\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcassandra\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Cluster\n\u001b[32m      4\u001b[39m cluster = Cluster([\u001b[33m'\u001b[39m\u001b[33m127.0.0.1\u001b[39m\u001b[33m'\u001b[39m])  \u001b[38;5;66;03m# Cassandra local\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m session = \u001b[43mcluster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcompanies_cassandra\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mAAPL_historico_MYsql.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m df.iterrows():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/novo/venv/lib/python3.12/site-packages/cassandra/cluster.py:1717\u001b[39m, in \u001b[36mcassandra.cluster.Cluster.connect\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/novo/venv/lib/python3.12/site-packages/cassandra/cluster.py:1753\u001b[39m, in \u001b[36mcassandra.cluster.Cluster.connect\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/novo/venv/lib/python3.12/site-packages/cassandra/cluster.py:1740\u001b[39m, in \u001b[36mcassandra.cluster.Cluster.connect\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/novo/venv/lib/python3.12/site-packages/cassandra/cluster.py:3543\u001b[39m, in \u001b[36mcassandra.cluster.ControlConnection.connect\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/novo/venv/lib/python3.12/site-packages/cassandra/cluster.py:3588\u001b[39m, in \u001b[36mcassandra.cluster.ControlConnection._reconnect_internal\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mNoHostAvailable\u001b[39m: ('Unable to connect to any servers', {'127.0.0.1:9042': ConnectionRefusedError(111, \"Tried connecting to [('127.0.0.1', 9042)]. Last error: Connection refused\")})"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from cassandra.cluster import Cluster\n",
    "\n",
    "cluster = Cluster(['127.0.0.1'])  # Cassandra local\n",
    "session = cluster.connect('companies_cassandra')\n",
    "\n",
    "df = pd.read_csv('AAPL_historico_MYsql.csv')\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    session.execute(\n",
    "        \"\"\"\n",
    "        INSERT INTO aapl_historico (date, close, high, low, open, volume, nome)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s)\n",
    "        \"\"\",\n",
    "        (row['Date'], row['Close'], row['High'], row['Low'], row['Open'], row['Volume'], row['nome'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de64cde5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bdsp2025)",
   "language": "python",
   "name": "venv-bdsp2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
