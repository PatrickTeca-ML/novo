{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3323ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee108525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import os\n",
    "#mysql_jar_path = os.path.abspath(\"mysql-connector-j-9.0.0.jar\")\n",
    "#print(\"Caminho completo do JAR:\", mysql_jar_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785d8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 04:33:13.849451: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-22 04:33:13.970340: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-22 04:33:14.080123: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747888394.161171    2904 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747888394.183244    2904 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747888394.364601    2904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747888394.364634    2904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747888394.364636    2904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747888394.364638    2904 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 04:33:14.386928: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "414f509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd640b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in ./venv/lib/python3.12/site-packages (0.2.61)\n",
      "Requirement already satisfied: pandas>=1.3.0 in ./venv/lib/python3.12/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.31 in ./venv/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in ./venv/lib/python3.12/site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in ./venv/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in ./venv/lib/python3.12/site-packages (from yfinance) (3.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in ./venv/lib/python3.12/site-packages (from yfinance) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.11.1)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in ./venv/lib/python3.12/site-packages (from yfinance) (5.29.4)\n",
      "Requirement already satisfied: websockets>=13.0 in ./venv/lib/python3.12/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
      "Requirement already satisfied: cffi>=1.12.0 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944da4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582e27cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price           Close       High        Low       Open     Volume\n",
      "Ticker           AAPL       AAPL       AAPL       AAPL       AAPL\n",
      "Date                                                             \n",
      "2018-01-02  40.426834  40.436224  39.722779  39.933998  102223600\n",
      "2018-01-03  40.419777  40.964248  40.356415  40.490183  118071600\n",
      "2018-01-04  40.607533  40.710794  40.384583  40.492536   89738400\n",
      "2018-01-05  41.069855  41.156687  40.612220  40.703747   94640000\n",
      "2018-01-08  40.917328  41.213030  40.818757  40.917328   82271200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ticker = \"AAPL\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ebe3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_AAPL = f\"{ticker}_AAPL_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_AAPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4284f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL= pd.read_csv(\"AAPL_AAPL_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8781aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>40.42683410644531</td>\n",
       "      <td>40.43622351118219</td>\n",
       "      <td>39.72277915354373</td>\n",
       "      <td>39.93399778801941</td>\n",
       "      <td>102223600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>40.419776916503906</td>\n",
       "      <td>40.964247570953226</td>\n",
       "      <td>40.35641457560373</td>\n",
       "      <td>40.490183098511274</td>\n",
       "      <td>118071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>40.6075325012207</td>\n",
       "      <td>40.71079444172855</td>\n",
       "      <td>40.3845825374921</td>\n",
       "      <td>40.49253559819918</td>\n",
       "      <td>89738400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price               Close                High                Low  \\\n",
       "0      Ticker                AAPL                AAPL               AAPL   \n",
       "1        Date                 NaN                 NaN                NaN   \n",
       "2  2018-01-02   40.42683410644531   40.43622351118219  39.72277915354373   \n",
       "3  2018-01-03  40.419776916503906  40.964247570953226  40.35641457560373   \n",
       "4  2018-01-04    40.6075325012207   40.71079444172855   40.3845825374921   \n",
       "\n",
       "                 Open     Volume  \n",
       "0                AAPL       AAPL  \n",
       "1                 NaN        NaN  \n",
       "2   39.93399778801941  102223600  \n",
       "3  40.490183098511274  118071600  \n",
       "4   40.49253559819918   89738400  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AAPL.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0749348f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+-----------------+------------------+---------+\n",
      "|     Price|             Close|              High|              Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+-----------------+------------------+---------+\n",
      "|    Ticker|              AAPL|              AAPL|             AAPL|              AAPL|     AAPL|\n",
      "|      Date|              null|              null|             null|              null|     null|\n",
      "|2018-01-02| 40.42683410644531| 40.43622351118219|39.72277915354373| 39.93399778801941|102223600|\n",
      "|2018-01-03|40.419776916503906|40.964247570953226|40.35641457560373|40.490183098511274|118071600|\n",
      "|2018-01-04|  40.6075325012207| 40.71079444172855| 40.3845825374921| 40.49253559819918| 89738400|\n",
      "+----------+------------------+------------------+-----------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Criar nova SparkSession com o caminho exato do JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do CSV\n",
    "file_path = \"file:///home/hduser/novo/AAPL_AAPL_historical_train_data_spark.csv\"\n",
    "\n",
    "# Leitura do CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Visualizar as primeiras 5 linhas\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1410318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab506793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+-----------------+------------------+---------+\n",
      "|     Price|             Close|              High|              Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+-----------------+------------------+---------+\n",
      "|    Ticker|              AAPL|              AAPL|             AAPL|              AAPL|     AAPL|\n",
      "|      Date|              null|              null|             null|              null|     null|\n",
      "|2018-01-02| 40.42683410644531| 40.43622351118219|39.72277915354373| 39.93399778801941|102223600|\n",
      "|2018-01-03|40.419776916503906|40.964247570953226|40.35641457560373|40.490183098511274|118071600|\n",
      "|2018-01-04|  40.6075325012207| 40.71079444172855| 40.3845825374921| 40.49253559819918| 89738400|\n",
      "+----------+------------------+------------------+-----------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab3274be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9685426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3960943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|      Date|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|2018-01-02| 40.42683410644531| 40.43622351118219| 39.72277915354373| 39.93399778801941|102223600|\n",
      "|2018-01-03|40.419776916503906|40.964247570953226| 40.35641457560373|40.490183098511274|118071600|\n",
      "|2018-01-04|  40.6075325012207| 40.71079444172855|  40.3845825374921| 40.49253559819918| 89738400|\n",
      "|2018-01-05|41.069854736328125|41.156686997562645|40.612219928324755|40.703746889925426| 94640000|\n",
      "|2018-01-08|40.917327880859375| 41.21302966783653| 40.81875703752503|40.917327880859375| 82271200|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95d194d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35d95115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89b561a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1d99cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e204efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
    "df = df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
    "df = df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
    "df = df.withColumn(\"Volume\", col(\"Volume\").cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5822132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f03d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "581cf41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+----------+\n",
      "|      Date|             Close|              High|               Low|              Open|    Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+----------+\n",
      "|2018-01-02| 40.42683410644531| 40.43622351118219| 39.72277915354373| 39.93399778801941|1.022236E8|\n",
      "|2018-01-03|40.419776916503906|40.964247570953226| 40.35641457560373|40.490183098511274|1.180716E8|\n",
      "|2018-01-04|  40.6075325012207| 40.71079444172855|  40.3845825374921| 40.49253559819918| 8.97384E7|\n",
      "|2018-01-05|41.069854736328125|41.156686997562645|40.612219928324755|40.703746889925426|   9.464E7|\n",
      "|2018-01-08|40.917327880859375| 41.21302966783653| 40.81875703752503|40.917327880859375| 8.22712E7|\n",
      "+----------+------------------+------------------+------------------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17fdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16b187a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ee5abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Leitura do CSV\n",
    "#df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Adiciona a coluna 'nome' com valor fixo \"AAPL\"\n",
    "df_comp_nome = df.withColumn(\"nome\", lit(\"AAPL\"))\n",
    "\n",
    "# Escreve no banco MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"AAPL_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2777b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163831",
   "metadata": {},
   "source": [
    "mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "443fb477",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pymysql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b72c52bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/971793236.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM AAPL_historico\", conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='password',\n",
    "    database='companies'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM AAPL_historico\", conn)\n",
    "df.to_csv(\"AAPL_historico_MYsql.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b1259451",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AAPL_Mysql= pd.read_csv(\"AAPL_historico_MYsql.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ea6aae2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>nome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>40.426834</td>\n",
       "      <td>40.436224</td>\n",
       "      <td>39.722779</td>\n",
       "      <td>39.933998</td>\n",
       "      <td>102223600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>40.419777</td>\n",
       "      <td>40.964248</td>\n",
       "      <td>40.356415</td>\n",
       "      <td>40.490183</td>\n",
       "      <td>118071600.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>40.607533</td>\n",
       "      <td>40.710794</td>\n",
       "      <td>40.384583</td>\n",
       "      <td>40.492536</td>\n",
       "      <td>89738400.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>41.069855</td>\n",
       "      <td>41.156687</td>\n",
       "      <td>40.612220</td>\n",
       "      <td>40.703747</td>\n",
       "      <td>94640000.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>40.917328</td>\n",
       "      <td>41.213030</td>\n",
       "      <td>40.818757</td>\n",
       "      <td>40.917328</td>\n",
       "      <td>82271200.0</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Close       High        Low       Open       Volume  nome\n",
       "0  2018-01-02  40.426834  40.436224  39.722779  39.933998  102223600.0  AAPL\n",
       "1  2018-01-03  40.419777  40.964248  40.356415  40.490183  118071600.0  AAPL\n",
       "2  2018-01-04  40.607533  40.710794  40.384583  40.492536   89738400.0  AAPL\n",
       "3  2018-01-05  41.069855  41.156687  40.612220  40.703747   94640000.0  AAPL\n",
       "4  2018-01-08  40.917328  41.213030  40.818757  40.917328   82271200.0  AAPL"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AAPL_Mysql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b7c46",
   "metadata": {},
   "source": [
    "# ABNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de64cde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open    Volume\n",
      "Ticker            ABNB        ABNB        ABNB        ABNB      ABNB\n",
      "Date                                                                \n",
      "2020-12-10  144.710007  165.000000  141.250000  146.000000  70447500\n",
      "2020-12-11  139.250000  151.500000  135.100006  146.550003  26980800\n",
      "2020-12-14  130.000000  135.300003  125.160004  135.000000  16966100\n",
      "2020-12-15  124.800003  127.599998  121.500000  126.690002  10914400\n",
      "2020-12-16  137.990005  142.000000  124.910004  125.830002  20409600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = \"ABNB\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8bfdd7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_ABNB = f\"{ticker}_ABNB_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_ABNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c4e47e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ABNB= pd.read_csv(\"ABNB_ABNB_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5ab8dd59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>ABNB</td>\n",
       "      <td>ABNB</td>\n",
       "      <td>ABNB</td>\n",
       "      <td>ABNB</td>\n",
       "      <td>ABNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>144.7100067138672</td>\n",
       "      <td>165.0</td>\n",
       "      <td>141.25</td>\n",
       "      <td>146.0</td>\n",
       "      <td>70447500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>139.25</td>\n",
       "      <td>151.5</td>\n",
       "      <td>135.10000610351562</td>\n",
       "      <td>146.5500030517578</td>\n",
       "      <td>26980800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>130.0</td>\n",
       "      <td>135.3000030517578</td>\n",
       "      <td>125.16000366210938</td>\n",
       "      <td>135.0</td>\n",
       "      <td>16966100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price              Close               High                 Low  \\\n",
       "0      Ticker               ABNB               ABNB                ABNB   \n",
       "1        Date                NaN                NaN                 NaN   \n",
       "2  2020-12-10  144.7100067138672              165.0              141.25   \n",
       "3  2020-12-11             139.25              151.5  135.10000610351562   \n",
       "4  2020-12-14              130.0  135.3000030517578  125.16000366210938   \n",
       "\n",
       "                Open    Volume  \n",
       "0               ABNB      ABNB  \n",
       "1                NaN       NaN  \n",
       "2              146.0  70447500  \n",
       "3  146.5500030517578  26980800  \n",
       "4              135.0  16966100  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ABNB.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff9e5a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+------------------+-----------------+--------+\n",
      "|     Price|            Close|             High|               Low|             Open|  Volume|\n",
      "+----------+-----------------+-----------------+------------------+-----------------+--------+\n",
      "|    Ticker|             ABNB|             ABNB|              ABNB|             ABNB|    ABNB|\n",
      "|      Date|             null|             null|              null|             null|    null|\n",
      "|2020-12-10|144.7100067138672|            165.0|            141.25|            146.0|70447500|\n",
      "|2020-12-11|           139.25|            151.5|135.10000610351562|146.5500030517578|26980800|\n",
      "|2020-12-14|            130.0|135.3000030517578|125.16000366210938|            135.0|16966100|\n",
      "+----------+-----------------+-----------------+------------------+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Criar nova SparkSession com o caminho exato do JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do CSV\n",
    "file_path = \"file:///home/hduser/novo/ABNB_ABNB_historical_train_data_spark.csv\"\n",
    "\n",
    "# Leitura do CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Visualizar as primeiras 5 linhas\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30f84445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b5f2d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------------+------------------+-----------------+--------+\n",
      "|     Price|            Close|             High|               Low|             Open|  Volume|\n",
      "+----------+-----------------+-----------------+------------------+-----------------+--------+\n",
      "|    Ticker|             ABNB|             ABNB|              ABNB|             ABNB|    ABNB|\n",
      "|      Date|             null|             null|              null|             null|    null|\n",
      "|2020-12-10|144.7100067138672|            165.0|            141.25|            146.0|70447500|\n",
      "|2020-12-11|           139.25|            151.5|135.10000610351562|146.5500030517578|26980800|\n",
      "|2020-12-14|            130.0|135.3000030517578|125.16000366210938|            135.0|16966100|\n",
      "+----------+-----------------+-----------------+------------------+-----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e6fd3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "931b7de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8ea2e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|      Date|             Close|             High|               Low|              Open|  Volume|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|2020-12-10| 144.7100067138672|            165.0|            141.25|             146.0|70447500|\n",
      "|2020-12-11|            139.25|            151.5|135.10000610351562| 146.5500030517578|26980800|\n",
      "|2020-12-14|             130.0|135.3000030517578|125.16000366210938|             135.0|16966100|\n",
      "|2020-12-15|124.80000305175781|127.5999984741211|             121.5|126.69000244140625|10914400|\n",
      "|2020-12-16|137.99000549316406|            142.0|124.91000366210938|125.83000183105469|20409600|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8af60c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1501701c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f2f7de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4eabf3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cf86993d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5b87f53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b81ca9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|      Date|             Close|             High|               Low|              Open|  Volume|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|2020-12-10| 144.7100067138672|            165.0|            141.25|             146.0|70447500|\n",
      "|2020-12-11|            139.25|            151.5|135.10000610351562| 146.5500030517578|26980800|\n",
      "|2020-12-14|             130.0|135.3000030517578|125.16000366210938|             135.0|16966100|\n",
      "|2020-12-15|124.80000305175781|127.5999984741211|             121.5|126.69000244140625|10914400|\n",
      "|2020-12-16|137.99000549316406|            142.0|124.91000366210938|125.83000183105469|20409600|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "71560dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "53cc33b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "548a7717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leitura do CSV\n",
    "#df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Adiciona a coluna 'nome' com valor fixo \"AAPL\"\n",
    "df_comp_nome = df.withColumn(\"nome\", lit(\"ABNB\"))\n",
    "\n",
    "# Escreve no banco MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"ABNB_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "00a4847d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f5d1f5",
   "metadata": {},
   "source": [
    "## mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f48fbdf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/3451849036.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM ABNB_historico\", conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='password',\n",
    "    database='companies'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM ABNB_historico\", conn)\n",
    "df.to_csv(\"ABNB_historico_MYsql.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2c1fc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ABNB_Mysql= pd.read_csv(\"ABNB_historico_MYsql.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c794e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>nome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>144.710007</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>141.250000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>70447500</td>\n",
       "      <td>ABNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-12-11</td>\n",
       "      <td>139.250000</td>\n",
       "      <td>151.500000</td>\n",
       "      <td>135.100006</td>\n",
       "      <td>146.550003</td>\n",
       "      <td>26980800</td>\n",
       "      <td>ABNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>135.300003</td>\n",
       "      <td>125.160004</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>16966100</td>\n",
       "      <td>ABNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>124.800003</td>\n",
       "      <td>127.599998</td>\n",
       "      <td>121.500000</td>\n",
       "      <td>126.690002</td>\n",
       "      <td>10914400</td>\n",
       "      <td>ABNB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-16</td>\n",
       "      <td>137.990005</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>124.910004</td>\n",
       "      <td>125.830002</td>\n",
       "      <td>20409600</td>\n",
       "      <td>ABNB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Close        High         Low        Open    Volume  nome\n",
       "0  2020-12-10  144.710007  165.000000  141.250000  146.000000  70447500  ABNB\n",
       "1  2020-12-11  139.250000  151.500000  135.100006  146.550003  26980800  ABNB\n",
       "2  2020-12-14  130.000000  135.300003  125.160004  135.000000  16966100  ABNB\n",
       "3  2020-12-15  124.800003  127.599998  121.500000  126.690002  10914400  ABNB\n",
       "4  2020-12-16  137.990005  142.000000  124.910004  125.830002  20409600  ABNB"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ABNB_Mysql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f191a6e",
   "metadata": {},
   "source": [
    "# AMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8f87a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open   Volume\n",
      "Ticker             AMT         AMT         AMT         AMT      AMT\n",
      "Date                                                               \n",
      "2018-01-02  118.035645  119.783636  117.851644  119.733457  1880600\n",
      "2018-01-03  118.495644  119.181466  117.809834  117.968745  1770500\n",
      "2018-01-04  116.764366  118.729820  116.739276  118.294908  1729600\n",
      "2018-01-05  117.517075  117.675985  116.747624  117.282895  2119400\n",
      "2018-01-08  118.771637  119.800370  117.642557  117.642557  1882100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ticker = \"AMT\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cf110a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_AMT = f\"{ticker}_AMT_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_AMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bc3f340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMT= pd.read_csv(\"AMT_AMT_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ce232d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "      <td>AMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>118.03564453125</td>\n",
       "      <td>119.78363614377574</td>\n",
       "      <td>117.85164407078862</td>\n",
       "      <td>119.73345652035125</td>\n",
       "      <td>1880600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>118.49564361572266</td>\n",
       "      <td>119.18146589604466</td>\n",
       "      <td>117.80983409725926</td>\n",
       "      <td>117.96874476035136</td>\n",
       "      <td>1770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>116.76436614990234</td>\n",
       "      <td>118.72981971286907</td>\n",
       "      <td>116.7392763394693</td>\n",
       "      <td>118.29490839560174</td>\n",
       "      <td>1729600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price               Close                High                 Low  \\\n",
       "0      Ticker                 AMT                 AMT                 AMT   \n",
       "1        Date                 NaN                 NaN                 NaN   \n",
       "2  2018-01-02     118.03564453125  119.78363614377574  117.85164407078862   \n",
       "3  2018-01-03  118.49564361572266  119.18146589604466  117.80983409725926   \n",
       "4  2018-01-04  116.76436614990234  118.72981971286907   116.7392763394693   \n",
       "\n",
       "                 Open   Volume  \n",
       "0                 AMT      AMT  \n",
       "1                 NaN      NaN  \n",
       "2  119.73345652035125  1880600  \n",
       "3  117.96874476035136  1770500  \n",
       "4  118.29490839560174  1729600  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AMT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a5b7f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|               AMT|               AMT|               AMT|               AMT|    AMT|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02|   118.03564453125|119.78363614377574|117.85164407078862|119.73345652035125|1880600|\n",
      "|2018-01-03|118.49564361572266|119.18146589604466|117.80983409725926|117.96874476035136|1770500|\n",
      "|2018-01-04|116.76436614990234|118.72981971286907| 116.7392763394693|118.29490839560174|1729600|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Criar nova SparkSession com o caminho exato do JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do CSV\n",
    "file_path = \"file:///home/hduser/novo/AMT_AMT_historical_train_data_spark.csv\"\n",
    "\n",
    "# Leitura do CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Visualizar as primeiras 5 linhas\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1fe563f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6633144f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|               AMT|               AMT|               AMT|               AMT|    AMT|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02|   118.03564453125|119.78363614377574|117.85164407078862|119.73345652035125|1880600|\n",
      "|2018-01-03|118.49564361572266|119.18146589604466|117.80983409725926|117.96874476035136|1770500|\n",
      "|2018-01-04|116.76436614990234|118.72981971286907| 116.7392763394693|118.29490839560174|1729600|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fdef9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a1422ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0ad5ecb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|      Date|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|2018-01-02|   118.03564453125|119.78363614377574|117.85164407078862|119.73345652035125|1880600|\n",
      "|2018-01-03|118.49564361572266|119.18146589604466|117.80983409725926|117.96874476035136|1770500|\n",
      "|2018-01-04|116.76436614990234|118.72981971286907| 116.7392763394693|118.29490839560174|1729600|\n",
      "|2018-01-05|117.51707458496094|117.67598521019036|116.74762402866821|117.28289453573514|2119400|\n",
      "|2018-01-08|118.77163696289062|119.80037031955236|117.64255711840566|117.64255711840566|1882100|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd82c6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "83549805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa7a762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a2f70c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1c1fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
    "df = df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
    "df = df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
    "df = df.withColumn(\"Volume\", col(\"Volume\").cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "336acf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "59285b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "65b718b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|      Date|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|2018-01-02|   118.03564453125|119.78363614377574|117.85164407078862|119.73345652035125|1880600.0|\n",
      "|2018-01-03|118.49564361572266|119.18146589604466|117.80983409725926|117.96874476035136|1770500.0|\n",
      "|2018-01-04|116.76436614990234|118.72981971286907| 116.7392763394693|118.29490839560174|1729600.0|\n",
      "|2018-01-05|117.51707458496094|117.67598521019036|116.74762402866821|117.28289453573514|2119400.0|\n",
      "|2018-01-08|118.77163696289062|119.80037031955236|117.64255711840566|117.64255711840566|1882100.0|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6fc5797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea307415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "36a176bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Leitura do CSV\n",
    "#df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Adiciona a coluna 'nome' com valor fixo \"AAPL\"\n",
    "df_comp_nome = df.withColumn(\"nome\", lit(\"AMT\"))\n",
    "\n",
    "# Escreve no banco MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"AMT_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4cdc4318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bef2c43",
   "metadata": {},
   "source": [
    "### mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f3ca77c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/2382641269.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM AMT_historico\", conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='password',\n",
    "    database='companies'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM AMT_historico\", conn)\n",
    "df.to_csv(\"AMT_historico_MYsql.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35f91934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMT_Mysql= pd.read_csv(\"AMT_historico_MYsql.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "512c04a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>nome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>118.035645</td>\n",
       "      <td>119.783636</td>\n",
       "      <td>117.851644</td>\n",
       "      <td>119.733457</td>\n",
       "      <td>1880600.0</td>\n",
       "      <td>AMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>118.495644</td>\n",
       "      <td>119.181466</td>\n",
       "      <td>117.809834</td>\n",
       "      <td>117.968745</td>\n",
       "      <td>1770500.0</td>\n",
       "      <td>AMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>116.764366</td>\n",
       "      <td>118.729820</td>\n",
       "      <td>116.739276</td>\n",
       "      <td>118.294908</td>\n",
       "      <td>1729600.0</td>\n",
       "      <td>AMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>117.517075</td>\n",
       "      <td>117.675985</td>\n",
       "      <td>116.747624</td>\n",
       "      <td>117.282895</td>\n",
       "      <td>2119400.0</td>\n",
       "      <td>AMT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>118.771637</td>\n",
       "      <td>119.800370</td>\n",
       "      <td>117.642557</td>\n",
       "      <td>117.642557</td>\n",
       "      <td>1882100.0</td>\n",
       "      <td>AMT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date       Close        High         Low        Open     Volume nome\n",
       "0  2018-01-02  118.035645  119.783636  117.851644  119.733457  1880600.0  AMT\n",
       "1  2018-01-03  118.495644  119.181466  117.809834  117.968745  1770500.0  AMT\n",
       "2  2018-01-04  116.764366  118.729820  116.739276  118.294908  1729600.0  AMT\n",
       "3  2018-01-05  117.517075  117.675985  116.747624  117.282895  2119400.0  AMT\n",
       "4  2018-01-08  118.771637  119.800370  117.642557  117.642557  1882100.0  AMT"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AMT_Mysql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f7968c",
   "metadata": {},
   "source": [
    "# AMZN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7e077104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price           Close       High        Low       Open    Volume\n",
      "Ticker           AMZN       AMZN       AMZN       AMZN      AMZN\n",
      "Date                                                            \n",
      "2018-01-02  59.450500  59.500000  58.525501  58.599998  53890000\n",
      "2018-01-03  60.209999  60.274502  59.415001  59.415001  62176000\n",
      "2018-01-04  60.479500  60.793499  60.233002  60.250000  60442000\n",
      "2018-01-05  61.457001  61.457001  60.500000  60.875500  70894000\n",
      "2018-01-08  62.343498  62.653999  61.601501  61.799999  85590000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ticker = \"AMZN\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ec9db76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_AMZN = f\"{ticker}_AMZN_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_AMZN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c66be87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMZN= pd.read_csv(\"AMZN_AMZN_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05f06286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>59.45050048828125</td>\n",
       "      <td>59.5</td>\n",
       "      <td>58.5255012512207</td>\n",
       "      <td>58.599998474121094</td>\n",
       "      <td>53890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>60.209999084472656</td>\n",
       "      <td>60.27450180053711</td>\n",
       "      <td>59.415000915527344</td>\n",
       "      <td>59.415000915527344</td>\n",
       "      <td>62176000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>60.47949981689453</td>\n",
       "      <td>60.79349899291992</td>\n",
       "      <td>60.233001708984375</td>\n",
       "      <td>60.25</td>\n",
       "      <td>60442000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price               Close               High                 Low  \\\n",
       "0      Ticker                AMZN               AMZN                AMZN   \n",
       "1        Date                 NaN                NaN                 NaN   \n",
       "2  2018-01-02   59.45050048828125               59.5    58.5255012512207   \n",
       "3  2018-01-03  60.209999084472656  60.27450180053711  59.415000915527344   \n",
       "4  2018-01-04   60.47949981689453  60.79349899291992  60.233001708984375   \n",
       "\n",
       "                 Open    Volume  \n",
       "0                AMZN      AMZN  \n",
       "1                 NaN       NaN  \n",
       "2  58.599998474121094  53890000  \n",
       "3  59.415000915527344  62176000  \n",
       "4               60.25  60442000  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AMZN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67943eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|     Price|             Close|             High|               Low|              Open|  Volume|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|    Ticker|              AMZN|             AMZN|              AMZN|              AMZN|    AMZN|\n",
      "|      Date|              null|             null|              null|              null|    null|\n",
      "|2018-01-02| 59.45050048828125|             59.5|  58.5255012512207|58.599998474121094|53890000|\n",
      "|2018-01-03|60.209999084472656|60.27450180053711|59.415000915527344|59.415000915527344|62176000|\n",
      "|2018-01-04| 60.47949981689453|60.79349899291992|60.233001708984375|             60.25|60442000|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Criar nova SparkSession com o caminho exato do JAR\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Caminho do CSV\n",
    "file_path = \"file:///home/hduser/novo/AMZN_AMZN_historical_train_data_spark.csv\"\n",
    "\n",
    "# Leitura do CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Visualizar as primeiras 5 linhas\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e9de1e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0595baca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|     Price|             Close|             High|               Low|              Open|  Volume|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "|    Ticker|              AMZN|             AMZN|              AMZN|              AMZN|    AMZN|\n",
      "|      Date|              null|             null|              null|              null|    null|\n",
      "|2018-01-02| 59.45050048828125|             59.5|  58.5255012512207|58.599998474121094|53890000|\n",
      "|2018-01-03|60.209999084472656|60.27450180053711|59.415000915527344|59.415000915527344|62176000|\n",
      "|2018-01-04| 60.47949981689453|60.79349899291992|60.233001708984375|             60.25|60442000|\n",
      "+----------+------------------+-----------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "785b2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a2da17e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1c3b9d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+\n",
      "|      Date|             Close|              High|               Low|              Open|  Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+\n",
      "|2018-01-02| 59.45050048828125|              59.5|  58.5255012512207|58.599998474121094|53890000|\n",
      "|2018-01-03|60.209999084472656| 60.27450180053711|59.415000915527344|59.415000915527344|62176000|\n",
      "|2018-01-04| 60.47949981689453| 60.79349899291992|60.233001708984375|             60.25|60442000|\n",
      "|2018-01-05|61.457000732421875|61.457000732421875|              60.5|  60.8754997253418|70894000|\n",
      "|2018-01-08| 62.34349822998047| 62.65399932861328| 61.60150146484375| 61.79999923706055|85590000|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d3cfc6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "87683c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0412750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9aab5ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "66344c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
    "df = df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
    "df = df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
    "df = df.withColumn(\"Volume\", col(\"Volume\").cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "70092a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "66ba6869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "587604fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+--------+\n",
      "|      Date|             Close|              High|               Low|              Open|  Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+\n",
      "|2018-01-02| 59.45050048828125|              59.5|  58.5255012512207|58.599998474121094| 5.389E7|\n",
      "|2018-01-03|60.209999084472656| 60.27450180053711|59.415000915527344|59.415000915527344|6.2176E7|\n",
      "|2018-01-04| 60.47949981689453| 60.79349899291992|60.233001708984375|             60.25|6.0442E7|\n",
      "|2018-01-05|61.457000732421875|61.457000732421875|              60.5|  60.8754997253418|7.0894E7|\n",
      "|2018-01-08| 62.34349822998047| 62.65399932861328| 61.60150146484375| 61.79999923706055| 8.559E7|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8b02d4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8adee10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection properties\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "843f14a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Leitura do CSV\n",
    "#df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Adiciona a coluna 'nome' com valor fixo \"AAPL\"\n",
    "df_comp_nome = df.withColumn(\"nome\", lit(\"AAPL\"))\n",
    "\n",
    "# Escreve no banco MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"AAPL_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "cb2ceb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "aca8f4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/2508196468.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM AMZN_historico\", conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='password',\n",
    "    database='companies'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM AMZN_historico\", conn)\n",
    "df.to_csv(\"AMZN_historico_MYsql.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "de099032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_AMZN_Mysql= pd.read_csv(\"AMZN_historico_MYsql.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14a59a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "      <th>nome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>59.450500</td>\n",
       "      <td>59.500000</td>\n",
       "      <td>58.525501</td>\n",
       "      <td>58.599998</td>\n",
       "      <td>53890000.0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>60.209999</td>\n",
       "      <td>60.274502</td>\n",
       "      <td>59.415001</td>\n",
       "      <td>59.415001</td>\n",
       "      <td>62176000.0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>60.479500</td>\n",
       "      <td>60.793499</td>\n",
       "      <td>60.233002</td>\n",
       "      <td>60.250000</td>\n",
       "      <td>60442000.0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>61.457001</td>\n",
       "      <td>61.457001</td>\n",
       "      <td>60.500000</td>\n",
       "      <td>60.875500</td>\n",
       "      <td>70894000.0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>62.343498</td>\n",
       "      <td>62.653999</td>\n",
       "      <td>61.601501</td>\n",
       "      <td>61.799999</td>\n",
       "      <td>85590000.0</td>\n",
       "      <td>AMZN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Close       High        Low       Open      Volume  nome\n",
       "0  2018-01-02  59.450500  59.500000  58.525501  58.599998  53890000.0  AMZN\n",
       "1  2018-01-03  60.209999  60.274502  59.415001  59.415001  62176000.0  AMZN\n",
       "2  2018-01-04  60.479500  60.793499  60.233002  60.250000  60442000.0  AMZN\n",
       "3  2018-01-05  61.457001  61.457001  60.500000  60.875500  70894000.0  AMZN\n",
       "4  2018-01-08  62.343498  62.653999  61.601501  61.799999  85590000.0  AMZN"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AMZN_Mysql.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27adadba",
   "metadata": {},
   "source": [
    "# BA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2d8ad2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open   Volume\n",
      "Ticker              BA          BA          BA          BA       BA\n",
      "Date                                                               \n",
      "2018-01-02  282.886383  283.029326  281.514071  281.847624  2978900\n",
      "2018-01-03  283.801300  284.468407  281.580839  282.028747  3211200\n",
      "2018-01-04  282.724426  284.392164  281.580823  283.934717  4171700\n",
      "2018-01-05  294.322296  294.369963  282.772028  282.819666  6177700\n",
      "2018-01-08  295.570709  296.247326  291.377547  294.150759  4124900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ticker = \"BA\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "61361f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_BA = f\"{ticker}_BA_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_BA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d18f2ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BA= pd.read_csv(\"BA_BA_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6646aaae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Df_BA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mDf_BA\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'Df_BA' is not defined"
     ]
    }
   ],
   "source": [
    "Df_BA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d1d92583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|                BA|                BA|                BA|                BA|     BA|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02| 282.8863830566406| 283.0293261619516|   281.51407107959| 281.8476243806707|2978900|\n",
      "|2018-01-03| 283.8013000488281|284.46840677330397|281.58083918445425|282.02874705190794|3211200|\n",
      "|2018-01-04|282.72442626953125|284.39216390251653| 281.5808231165554| 283.9347168247189|4171700|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# CSV PATH\n",
    "file_path = \"file:///home/hduser/novo/BA_BA_historical_train_data_spark.csv\"\n",
    "\n",
    "# READING OF THE CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b20afe7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0478452d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|                BA|                BA|                BA|                BA|     BA|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02| 282.8863830566406| 283.0293261619516|   281.51407107959| 281.8476243806707|2978900|\n",
      "|2018-01-03| 283.8013000488281|284.46840677330397|281.58083918445425|282.02874705190794|3211200|\n",
      "|2018-01-04|282.72442626953125|284.39216390251653| 281.5808231165554| 283.9347168247189|4171700|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c38294c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "868be1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "305b6631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|      Date|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|2018-01-02| 282.8863830566406| 283.0293261619516|   281.51407107959| 281.8476243806707|2978900|\n",
      "|2018-01-03| 283.8013000488281|284.46840677330397|281.58083918445425|282.02874705190794|3211200|\n",
      "|2018-01-04|282.72442626953125|284.39216390251653| 281.5808231165554| 283.9347168247189|4171700|\n",
      "|2018-01-05| 294.3222961425781|294.36996323292874|  282.772027931123|282.81966593844106|6177700|\n",
      "|2018-01-08| 295.5707092285156| 296.2473259689747|291.37754675172033|294.15075927209307|4124900|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32289cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a49404e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1eeb514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ec9ba26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2c244b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
    "df = df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
    "df = df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
    "df = df.withColumn(\"Volume\", col(\"Volume\").cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c6bbc78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "09abb860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6d0ba5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|      Date|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|2018-01-02| 282.8863830566406| 283.0293261619516|   281.51407107959| 281.8476243806707|2978900.0|\n",
      "|2018-01-03| 283.8013000488281|284.46840677330397|281.58083918445425|282.02874705190794|3211200.0|\n",
      "|2018-01-04|282.72442626953125|284.39216390251653| 281.5808231165554| 283.9347168247189|4171700.0|\n",
      "|2018-01-05| 294.3222961425781|294.36996323292874|  282.772027931123|282.81966593844106|6177700.0|\n",
      "|2018-01-08| 295.5707092285156| 296.2473259689747|291.37754675172033|294.15075927209307|4124900.0|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d579e2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cbeb20e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "85d97209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_comp_nome = df.withColumn(\"nome\", lit(\"BA\"))\n",
    "\n",
    "# WRITE IN  MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"BA_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a467034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "79ee9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "## mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "45b0a052",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2904/3862318621.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(\"SELECT * FROM BA_historico\", conn)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='password',\n",
    "    database='companies'\n",
    ")\n",
    "\n",
    "df = pd.read_sql(\"SELECT * FROM BA_historico\", conn)\n",
    "df.to_csv(\"BA_historico_MYsql.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56aaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BA_Mysql= pd.read_csv(\"ABNB_historico_MYsql.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff9b424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bdsp2025)",
   "language": "python",
   "name": "venv-bdsp2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
