{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3323ef5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./venv/lib/python3.12/site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "785d8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-17 00:31:22.766901: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 00:31:22.886877: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-17 00:31:22.983457: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747441883.069154    3367 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747441883.092167    3367 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747441883.269917    3367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747441883.269944    3367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747441883.269946    3367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747441883.269947    3367 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-17 00:31:23.295007: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "414f509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression, RandomForestRegressor\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd640b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in ./venv/lib/python3.12/site-packages (0.2.61)\n",
      "Requirement already satisfied: pandas>=1.3.0 in ./venv/lib/python3.12/site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2.1.3)\n",
      "Requirement already satisfied: requests>=2.31 in ./venv/lib/python3.12/site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in ./venv/lib/python3.12/site-packages (from yfinance) (4.3.8)\n",
      "Requirement already satisfied: pytz>=2022.5 in ./venv/lib/python3.12/site-packages (from yfinance) (2025.2)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in ./venv/lib/python3.12/site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in ./venv/lib/python3.12/site-packages (from yfinance) (3.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in ./venv/lib/python3.12/site-packages (from yfinance) (4.13.4)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in ./venv/lib/python3.12/site-packages (from yfinance) (0.11.1)\n",
      "Requirement already satisfied: protobuf>=3.19.0 in ./venv/lib/python3.12/site-packages (from yfinance) (5.29.4)\n",
      "Requirement already satisfied: websockets>=13.0 in ./venv/lib/python3.12/site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in ./venv/lib/python3.12/site-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.2)\n",
      "Requirement already satisfied: cffi>=1.12.0 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in ./venv/lib/python3.12/site-packages (from curl_cffi>=0.7->yfinance) (2025.4.26)\n",
      "Requirement already satisfied: pycparser in ./venv/lib/python3.12/site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests>=2.31->yfinance) (2.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "944da4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582e27cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price            Close        High         Low        Open   Volume\n",
      "Ticker              BA          BA          BA          BA       BA\n",
      "Date                                                               \n",
      "2018-01-02  282.886444  283.029387  281.514132  281.847685  2978900\n",
      "2018-01-03  283.801208  284.468315  281.580748  282.028656  3211200\n",
      "2018-01-04  282.724396  284.392133  281.580793  283.934686  4171700\n",
      "2018-01-05  294.322266  294.369933  282.771999  282.819637  6177700\n",
      "2018-01-08  295.570770  296.247387  291.377607  294.150820  4124900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ticker = \"BA\"\n",
    "data = yf.download(ticker,\n",
    "                   start=\"2018-01-01\", \n",
    "                   end=\"2020-12-31\")    \n",
    "\n",
    "print(data.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebe3ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_BA = f\"{ticker}_BA_historical_train_data_spark.csv\"\n",
    "data.to_csv(csv_BA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4284f0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BA= pd.read_csv(\"BA_BA_historical_train_data_spark.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8781aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>Close</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ticker</td>\n",
       "      <td>BA</td>\n",
       "      <td>BA</td>\n",
       "      <td>BA</td>\n",
       "      <td>BA</td>\n",
       "      <td>BA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Date</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>282.8864440917969</td>\n",
       "      <td>283.02938722794903</td>\n",
       "      <td>281.5141318186582</td>\n",
       "      <td>281.84768519170586</td>\n",
       "      <td>2978900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>283.80120849609375</td>\n",
       "      <td>284.4683150053647</td>\n",
       "      <td>281.58074834802835</td>\n",
       "      <td>282.0286560709894</td>\n",
       "      <td>3211200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>282.7243957519531</td>\n",
       "      <td>284.392133204921</td>\n",
       "      <td>281.580792722419</td>\n",
       "      <td>283.93468617650075</td>\n",
       "      <td>4171700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Price               Close                High                 Low  \\\n",
       "0      Ticker                  BA                  BA                  BA   \n",
       "1        Date                 NaN                 NaN                 NaN   \n",
       "2  2018-01-02   282.8864440917969  283.02938722794903   281.5141318186582   \n",
       "3  2018-01-03  283.80120849609375   284.4683150053647  281.58074834802835   \n",
       "4  2018-01-04   282.7243957519531    284.392133204921    281.580792722419   \n",
       "\n",
       "                 Open   Volume  \n",
       "0                  BA       BA  \n",
       "1                 NaN      NaN  \n",
       "2  281.84768519170586  2978900  \n",
       "3   282.0286560709894  3211200  \n",
       "4  283.93468617650075  4171700  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_BA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0749348f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/17 00:33:09 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|                BA|                BA|                BA|                BA|     BA|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02| 282.8864440917969|283.02938722794903| 281.5141318186582|281.84768519170586|2978900|\n",
      "|2018-01-03|283.80120849609375| 284.4683150053647|281.58074834802835| 282.0286560709894|3211200|\n",
      "|2018-01-04| 282.7243957519531|  284.392133204921|  281.580792722419|283.93468617650075|4171700|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Parar sessão anterior, se necessário\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark CSV to MySQL\") \\\n",
    "    .master(\"local\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark-3.4.4-bin-hadoop3/jars/mysql-connector-j-8.0.33.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# CSV PATH\n",
    "file_path = \"file:///home/hduser/novo/BA_BA_historical_train_data_spark.csv\"\n",
    "\n",
    "# READING OF THE CSV\n",
    "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1410318",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the data type info\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab506793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|     Price|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|    Ticker|                BA|                BA|                BA|                BA|     BA|\n",
      "|      Date|              null|              null|              null|              null|   null|\n",
      "|2018-01-02| 282.8864440917969|283.02938722794903| 281.5141318186582|281.84768519170586|2978900|\n",
      "|2018-01-03|283.80120849609375| 284.4683150053647|281.58074834802835| 282.0286560709894|3211200|\n",
      "|2018-01-04| 282.7243957519531|  284.392133204921|  281.580792722419|283.93468617650075|4171700|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab3274be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the first two lines (CRL and null)\n",
    "df = df.filter(df.Price != \"Ticker\").filter(df.Price != \"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9685426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.toDF(\"Date\", \"Close\", \"High\", \"Low\", \"Open\", \"Volume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3960943",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|      Date|             Close|              High|               Low|              Open| Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "|2018-01-02| 282.8864440917969|283.02938722794903| 281.5141318186582|281.84768519170586|2978900|\n",
      "|2018-01-03|283.80120849609375| 284.4683150053647|281.58074834802835| 282.0286560709894|3211200|\n",
      "|2018-01-04| 282.7243957519531|  284.392133204921|  281.580792722419|283.93468617650075|4171700|\n",
      "|2018-01-05|     294.322265625|294.36993271040814|282.77199861116463|282.81963661354325|6177700|\n",
      "|2018-01-08| 295.5707702636719| 296.2473871438518| 291.3776069209913|294.15082001403056|4124900|\n",
      "+----------+------------------+------------------+------------------+------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95d194d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35d95115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89b561a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string double\n",
    "df = df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1d99cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e204efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
    "df = df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
    "df = df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
    "df = df.withColumn(\"Volume\", col(\"Volume\").cast(\"double\"))\n",
    "df = df.withColumn(\"Date\", col(\"Date\").cast(\"Date\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5822132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- Volume: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f03d497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3.0\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "print(mysql.connector.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "581cf41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|      Date|             Close|              High|               Low|              Open|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "|2018-01-02| 282.8864440917969|283.02938722794903| 281.5141318186582|281.84768519170586|2978900.0|\n",
      "|2018-01-03|283.80120849609375| 284.4683150053647|281.58074834802835| 282.0286560709894|3211200.0|\n",
      "|2018-01-04| 282.7243957519531|  284.392133204921|  281.580792722419|283.93468617650075|4171700.0|\n",
      "|2018-01-05|     294.322265625|294.36993271040814|282.77199861116463|282.81963661354325|6177700.0|\n",
      "|2018-01-08| 295.5707702636719| 296.2473871438518| 291.3776069209913|294.15082001403056|4124900.0|\n",
      "+----------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17fdecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16b187a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL connection\n",
    "url = \"jdbc:mysql://localhost:3306/companies\"  # MySQL URL\n",
    "properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"password\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ee5abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df_comp_nome = df.withColumn(\"nome\", lit(\"BA\"))\n",
    "\n",
    "# WRITE IN  MySQL\n",
    "df_comp_nome.write \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/companies\") \\\n",
    "    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "    .option(\"dbtable\", \"BA_historico\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"password\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2777b2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+---+----+------+\n",
      "|Date|Close|High|Low|Open|Volume|\n",
      "+----+-----+----+---+----+------+\n",
      "|   0|    0|   0|  0|   0|     0|\n",
      "+----+-----+----+---+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Count Number of nulls in all Column\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163831",
   "metadata": {},
   "source": [
    "mySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72c52bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bdsp2025)",
   "language": "python",
   "name": "venv-bdsp2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
